#!/usr/bin/env bash
#
set -x

# Benchmark info
echo "TIMING - Starting main script at: $(date)"

# Set working directory to home directory
cd "${HOME}"

# Benchmark info
echo "TIMING - Starting llama.cpp at: $(date)"



llm_model=<%= context.llm_model -%>

export num_cpus=20
echo MODEL: $llm_model
echo HOST $host

export LD_LIBRARY_PATH=/opt/ood-llm-aux/dev/llama.cpp/current/build_gpu/bin/:$LD_LIBRARY_PATH
export oodllm_aux_dir=/opt/ood-llm-aux/dev

echo "Starting proxy..."
oodproxy=${oodllm_aux_dir}/oodproxy/oodproxy
${oodproxy} --listen ${host}:${port} \
            --ipv4-range "193.156.40.0/22" \
            --ipv6-range "2001:700:5800::/41" \
            --model-script "
              ${oodllm_aux_dir}/runllm.sh
                ${llm_model}
                --threads ${num_cpus}
                --host /tmp/llamacpp.sock
              "

