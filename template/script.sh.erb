#!/usr/bin/env bash

# Benchmark info
echo "TIMING - Starting main script at: $(date)"

# Set working directory to home directory
cd "${HOME}"

# Purge the module environment to avoid conflicts
module purge

# Benchmark info
echo "TIMING - Starting llama.cpp at: $(date)"

model_source="<%= context.model_source %>"
llm_model="<%= context.llm_model %>"
model_path="<%= context.model_path %>"
num_cpus=<%= context.ec_cpus %>
num_gpus=<%= context.ec_gpus.to_i %>

echo "Model Source: ${model_source}"
echo "Model: ${llm_model}"
echo "Model Path: ${model_path}"
echo "Host: ${host}"
echo "CPUs: ${num_cpus}"
echo "GPUs: ${num_gpus}"

oodllm_aux_version=20251207_1b800bb
oodllm_aux_dir=/cluster/opt/OOD/ood-llm-aux/${oodllm_aux_version}
llamacpp_socket=/tmp/llamacpp.sock

echo "Starting proxy..."
oodproxy=${oodllm_aux_dir}/oodproxy/oodproxy

# Simple if-else: custom or predefined
if [[ "${model_source}" == "custom" ]]; then

  if [[ ! -f "${model_path}" ]]; then
    echo "ERROR: Model file not found: ${model_path}"
    exit 1
  fi
  
  module load CUDA/12.6.0
  module load GLib/2.80.4-GCCcore-13.3.0
  
  llamacpp_dir=${oodllm_aux_dir}/llama.cpp/current/
  
  # GPU or CPU build
  if [[ ${num_gpus} -gt 0 ]]; then
    llamacpp_srv=${llamacpp_dir}build_gpu/bin/llama-server
  else
    llamacpp_srv=${llamacpp_dir}build_cpu/bin/llama-server
  fi
  
  ${oodproxy} --listen ${host}:${port} \
              --ipv4-range "193.156.40.0/22" \
              --ipv6-range "2001:700:5800::/41" \
              --model-script "${llamacpp_srv} --model ${model_path} --threads ${num_cpus} --host ${llamacpp_socket}"
else
  # Predefined model
  module load CUDA/12.6.0
  
  ${oodproxy} --listen ${host}:${port} \
              --ipv4-range "193.156.40.0/22" \
              --ipv6-range "2001:700:5800::/41" \
              --model-script "
                ${oodllm_aux_dir}/runllm.sh
                  ${llm_model}
                  --threads ${num_cpus}
                  --host ${llamacpp_socket}
                "
fi

